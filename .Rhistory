IBOV = ts(IBOV, dates)
IBOV = ts(IBOV, dates, frequency = 1)
IBOV = ts(IBOV, index(dates))
IBOV
index(IBOV)
df = data.frame(IBOV, dates)
View(df)
rownames(IBOV)= dates
View(IBOV)
rownames(df) = dates
View(df)
df['dates'] = NULL
plot(df)
View(df)
plot(df['BVSP.Close'])
plot(df['BVSP.Close'])
View(df)
plot(IBOV, dates)
plot(dates, IBOV)
plot(dates, IBOV, type='l')
rm(df)
fit =  fitted(vol)
summary(vol)
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='Volatilidade do IBOV usando o GARCH(1,1)')
ar = arima(IBOV, order=c(1,1,1))
summary(ar)
ar
fit = fitted(ar)
fit =  fitted(vol)
fit2 = fitted(ar)
plot(fit2)
fit2 = fitted(ar, h=20)
fit2 = forecast(ar)
View(ar)
ar = auto.arima(IBOV, order=c(1,1,1))
#----------- ARIMA
library(tseries)
ar = auto.arima(IBOV, order=c(1,1,1))
library(timeSeries)
ar = auto.arima(IBOV, order=c(1,1,1))
ar = auto.arima(IBOV, lambda='auto')
library(forecast)
ar = auto.arima(IBOV, lambda='auto')
ar
ar = auto.arima(IBOV, c(1,1,1))
ar = auto.arima(IBOV, order=c(1,1,1))
ar = auto.arima(IBOV, start.p = 1)
auto.arima
ar
ar = auto.arima(IBOV, start.p = 1, start.q = 1)
ar
ar = auto.arima(IBOV, start.p = 1, start.q = 2)
ar
ar = auto.arima(IBOV, start.p = 3, start.q = 2)
ar
ar = auto.arima(IBOV, start.p = 2, start.q = 1)
ar
ar = auto.arima(IBOV, start.q = 1)
ar
ar
ar
ar = auto.arima(IBOV, start.q = 1)
ar
ar = auto.arima(IBOV, lambda = 'auto')
ar
fit = forecast(ar, h=10)
plot(fit)
plot(fit, dates)
library(tseries)
library(timeSeries)
library(forecast)   # auto.arima
library(quantmod)
library(fGarch)
getSymbols("^BVSP", src="yahoo", from="2010-01-01")
dates = index(BVSP)
IBOV = BVSP[1:2505 , 4]
View(IBOV)
View(IBOV)
plot( IBOV, type='l')
basicStats(IBOV)
ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV)
IBOV = ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV)
View(IBOV)
IBOV = BVSP[1:2505 , 4]
IBOV = xts(ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV))
#-------------- GARCH
adf.test(IBOV)
na.omit(IBOV)
IBOV= na.omit(IBOV)
basicStats(IBOV)
adf.test(IBOV)
IBOV[1]
IBOV[2]
IBOV[1:5]
IBOV['new'] = diff(IBOV)
n = length(IBOV)
for (i in n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
for (i in n){
print(i)
}
for (i in 1:n){
print(i)
}
n = length(IBOV)
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
n = length(IBOV)
ret = c( )
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
n = length(IBOV) + 1
ret = c( )
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
ret = c( )
for (i in 1:n) {
if (i+1 =< n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret = c( )
for (i in 1:n) {
if (i+1 =< n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret = c( )
for (i in 1:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret
ret = rep(0,n )
for (i in 1:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
for (i in 2:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
rm(ret)
rm(ret)
for (i in 2:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
for (i in 2:n) {
if (i+1 < n){
ret = log(IBOV[i+1]/IBOV[i])
}
}
View(ret)
for (i in 1:n) {
if (i+1 < n){
ret = log(IBOV[i+1]/IBOV[i])
}
}
log(IBOV[1]/IBOV[1])
log(IBOV[2]/IBOV[1])
log(1/2)
IBOV[1]
IBOV[1]/IBOV[2]
bvcore = coredata(BVSP)
View(bvcore)
class(BVSP)
bvcore = index(dates)
View(bvcore)
bvcore = coredata(BVSP)
colnames(bvcore) = dates
rownames(bvcore) = dates
View(bvcore)
rownames(bvcore) = dates
bvcore[1, 4]
bvcore[1:5, 4]
bvcore[1:5, 4:2]
bvcore[1:5, 2:4]
bvcore[1:5, :4]
bvcore[1:5, 4:4]
bvcore[1:5, 4]
bvcore[1:5, :4]
bvcore[1:5, 4:]
bvcore[1:, 4]
bvcore = data.frame(bvcore)
View(bvcore)
rownames(bvcore) =  dates
View(bvcore)
bvcore[1:4, 1:4]
bvcore[1:4, 4]
names(bvcore)[4] = "IBOV"
View(bvcore)
bvcore[1: , 4]
bvcore[1:4, 4]
rm(ret)
bvcore[1:2505, 4]
IBOV = bvcore[1:2505, 4]
View(IBOV)
rm(IBOV)
bvcore <- bvcore[ ,4, drop=FALSE]
View(bvcore)
bvcore
bvcore[1]
bvcore[1]
bvcore[1]
bvcore[1]
bvcore[1:2]
bvcore[1:2, 1]
bvcore[1:1, 1]
bvcore[1, 1]
bvcore[2, 1]
bvcore[3, 1]
bvcore[3, 1]/bvcore[2, 1]
log(bvcore[3, 1]/bvcore[2, 1])
for (i in 1:n) {
if (i+1 < n){
bvcore$ret = log(bvcore[i+1, 1]/bvcore[i, 1])
}
}
View(bvcore)
for (i in 1:n) {
#if (i+1 < n){
bvcore$ret = log(bvcore[i+1, 1]/bvcore[i, 1])
}
View(bvcore)
for (i in 1:n){
print(i)
}
for (i in 1:n){
print(i)
}
log(bvcore[3, 1]/bvcore[2, 1])
log(bvcore[4, 1]/bvcore[3, 1])
for (i in 1:n) {
#if (i+1 < n){
bvcore$ret[i+1] = log(bvcore[i+1, 1]/bvcore[i, 1])
}
View(bvcore)
plot(bvcore[, 2])
plot(bvcore[ , 2], type='l')
plot(bvcore[ , 2], type='l', col='blue')
plot(bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
)
plot(bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
shapiro.test(bvcore[ , 2])
View(bvcore)
plot(bvcore[ , 2], dates, type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
plot(dates,bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
hist(bvcore[ , 2])
hist(bvcore[ , 2], col = 'lightblue')
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
shapiro.test(bvcore[ , 2])
shapiro.test(bvcore[ , 2]) #The null-hypothesis of this test is that the population is normally distributed.
jarque.bera.test(bvcore[ , 2])
jarque.bera.test(omit.na(bvcore[ , 2]))
jarque.bera.test(na.omit(bvcore[ , 2]))
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
curve(dnorm(x),add=T)
hist(rnorm(1000))
curve(dnorm(x),add=T)
hist(rnorm(1000))
curve(dnorm(x),add=T)
hist(rnorm(1000), ylim=c(0,0.5))
curve(dnorm(x),add=T)
hist(rnorm(1000), probability = T)
curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', probability = T)
curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', density = T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', probability = T)
curve(dnorm(x),add=T)
basicStats(bvcore[ , 2])
t.test(bvcore[ , 2], mu=0)
adf.test(bvcore[ , 2])
adf.test(na.omit(bvcore[ , 2]))
adf.test(na.omit(bvcore[ , 2]) )
bvcore[ , 2] = ifelse(is.na(bvcore[ , 2]), mean(bvcore[ , 2], na.rm=T ), bvcore[ , 2])
View(bvcore)
basicStats(bvcore[ , 2])
vol = garchFit(bvcore[ , 2] ~garch(1, 1), trace = FALSE)
summary(vol)
basicStats(bvcore[ , 2])
summary(vol)
fit =  fitted(vol)
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='Volatilidade do IBOV usando o GARCH(1,1)')
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='IBOV volatility using GARCH(1,1)')
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
fit = forecast(ar, h=10)
plot(fit)
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
fit = forecast(ar, h=30)
plot(fit)
library(mFilter)
filt = hpfilter(bvcore[ , 2])
filt = hpfilter(bvcore[ , 2], freq = 1600)
plot(filt)
#----------- ARIMA
acf(bvcore[ , 2])
pacf(bvcore[ , 2])
par(mfrow=c(1,2))
acf(bvcore[ , 2])
pacf(bvcore[ , 2])
acf(bvcore[ , 2], main='Return IBOV')
#----------- ARIMA
par(mfrow=c(1,2))
acf(bvcore[ , 2], main='Autocorrelation of IBOV returns')
pacf(bvcore[ , 2], main='Partial autocorrelation of IBOV returns')
#----------- ARIMA
window()
#----------- ARIMA
windows()
windows()
par(mfrow=c(1,2))
acf(bvcore[ , 2], main='Autocorrelation of IBOV returns')
pacf(bvcore[ , 2], main='Partial autocorrelation of IBOV returns')
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
ar
getwd()
rep(10, 1)
rep(10, 10)
rep(10,-1, 10)
rep(10,2, 10)
rep(10, 2, 10)
c(1,10,2)
seq(1, 10, 2)
seq(2, 10, 2)
seq(2, 10, -2)
seq(20, 10, -2)
seq(20, 1, -2)
rnorm(1000)
t = rnorm(1000)
hist(t)
hist(t, color='lightblue')
hist(t, col='lightblue')
t = rnorm(1000)
hist(t, col='lightblue')
t = rnorm(1000)
hist(t, col='lightblue')
rm(t)
5/14*6/14
1/6
setwd("D:/Git projects/ML in R")
df = read.csv('credit.csv')
View(df)
setwd("D:/Git projects/ML in R")
df = read.csv('naive_base')
df = read.csv('naive_base.csv')
install.packages(e1071)
install.packages('e1071')
library(e1071)
df[ , 1:4]
df[1 , 1:4]
df[ , 1:5]
df[ , 1:4]
df[-5]
df[ , 5]
df[ , 5:5]
clas = naiveBayes(x=df[-5], y = df$risco)
View(clas)
print(class)
print(clas)
história = c('boa')
dívida = c('alta')
garantias =  c('nenhuma')
renda = c('acima_35')
df2 = data.frame(história, dívida, garantias, renda)
prev = predict(clas, newdata = df2)
prev
print(prev)
View(prev)
prev = predict(clas, newdata = df2, 'raw')
print(prev)
print(prev) # a probabilidade do cara ser caloteiro é baixa
df2 = data.frame(história, dívida, garantias, renda)
prev = predict(clas, newdata = df2, 'raw') # esse raw faz aparecer os valores das probabilidades
prev = predict(clas, newdata = df2, 'raw') # esse raw faz aparecer os valores das probabilidades
print(prev) # a probabilidade do cara ser caloteiro é baixa
View(df)
setwd("D:/Git projects/ML in R")
df = read.csv('credit.csv')
df$clientid = NULL
attach(df)
m_age = mean(age[age>0 ], na.rm = T)  # average without considering negative values
df$age = ifelse(df$age<0 | is.na(df$age), m_age, df$age)
attach(df)
basicStats(df$age)
summary(df$age)
library(fBasics)
basicStats(df$age)
library(e1071)
df[ , 1:3] = scale(df[ , 1:3])  #padronization (score Z)
library(caTools)
set.seed(1)
div = sample.split(df$default, SplitRatio = 0.75)
df_train = subset(df, div == T)
df_test = subset(df, div == F)
#--------------------------------------------------------------------------------
#                               NAIVE BAYES
#--------------------------------------------------------------------------------
setwd("D:/Git projects/ML in R")
library(fBasics)
library(e1071)
library(caTools)
df = read.csv('credit.csv')
df$clientid = NULL
attach(df)
m_age = mean(age[age>0 ], na.rm = T)  # average without considering negative values
df$age = ifelse(df$age < 0 |is.na(base$age , m_age, base$age)
# Escalonamento
df[, 1:3] = scale(base[, 1:3])
df$age = ifelse(df$age < 0 |is.na(base$age) , m_age, base$age)
df$age = ifelse(df$age < 0 |is.na(df$age) , m_age, df$age)
# Escalonamento
df[, 1:3] = scale(df[, 1:3])
View(df)
set.seed(1)
div = sample.split(df$income, SplitRatio = 0.75)
df_train = subset(df, divisao == TRUE)
df_test = subset(df, divisao == FALSE)
set.seed(1)
div = sample.split(df$income, SplitRatio = 0.75)
df_train = subset(df, div == TRUE)
df_test = subset(df, div == FALSE)
View(df)
View(df)
clas = naiveBayes(x= df_train[-4], y= df_train$default)
print(clas)
prev = predict(clas, newdata = df_test[-4])
prev
df$default = factor(df$default, levels = c(0,1))
set.seed(1)
div = sample.split(df$income, SplitRatio = 0.75)
df_train = subset(df, div == TRUE)
df_test = subset(df, div == FALSE)
clas = naiveBayes(x= df_train[-4], y= df_train$default)
print(clas)
# test
prev = predict(clas, newdata = df_test[-4])
prev
comp = prev == df_test$default
summary(comp)
conf_matrix = table(df_test$default, prev)
conf_matrix
install.packages(caret)
install.packages('caret')
library(caret)
confusionMatrix(conf_matrix)
df = read.csv('census.csv')
base = read.csv('census.csv')
df = base
rm(df)
base$X = NULL
attach(base)
base$sex = ifelse(df$sex==' Male', 1, 0)
unique(base$sex)
base$sex = ifelse(df$sex=='Male', 1, 0)
unique(base$workclass)
unique(base$workclass)
base$sex = factor(base$sex, levels = unique(base$sex), labels = c(1, 0))
base$workclass = factor(base$workclass, levels = c(' Federal-gov', ' Local-gov', ' Private', ' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'), labels = c(1, 2, 3, 4, 5, 6, 7))
base$education = factor(base$education, levels = c(' 10th', ' 11th', ' 12th', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th', ' Assoc-acdm', ' Assoc-voc', ' Bachelors', ' Doctorate', ' HS-grad', ' Masters', ' Preschool', ' Prof-school', ' Some-college'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16))
base$marital.status = factor(base$marital.status, levels = c(' Divorced', ' Married-AF-spouse', ' Married-civ-spouse', ' Married-spouse-absent', ' Never-married', ' Separated', ' Widowed'), labels = c(1, 2, 3, 4, 5, 6, 7))
base$occupation = factor(base$occupation, levels = c(' Adm-clerical', ' Armed-Forces', ' Craft-repair', ' Exec-managerial', ' Farming-fishing', ' Handlers-cleaners', ' Machine-op-inspct', ' Other-service', ' Priv-house-serv', ' Prof-specialty', ' Protective-serv', ' Sales', ' Tech-support', ' Transport-moving'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
base$relationship = factor(base$relationship, levels = c(' Husband', ' Not-in-family', ' Other-relative', ' Own-child', ' Unmarried', ' Wife'), labels = c(1, 2, 3, 4, 5, 6))
base$race = factor(base$race, levels = c(' Amer-Indian-Eskimo', ' Asian-Pac-Islander', ' Black', ' Other', ' White'), labels = c(1, 2, 3, 4, 5))
base$native.country = factor(base$native.country, levels = c(' Cambodia', ' Canada', ' China', ' Columbia', ' Cuba', ' Dominican-Republic', ' Ecuador', ' El-Salvador', ' England', ' France', ' Germany', ' Greece', ' Guatemala', ' Haiti', ' Holand-Netherlands', ' Honduras', ' Hong', ' Hungary', ' India', ' Iran', ' Ireland', ' Italy', ' Jamaica', ' Japan', ' Laos', ' Mexico', ' Nicaragua', ' Outlying-US(Guam-USVI-etc)', ' Peru', ' Philippines', ' Poland', ' Portugal', ' Puerto-Rico', ' Scotland', ' South', ' Taiwan', ' Thailand', ' Trinadad&Tobago', ' United-States', ' Vietnam', ' Yugoslavia'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41))
base$income = factor(base$income, levels = c(' <=50K', ' >50K'), labels = c(0, 1))
base[ , 1] = scale(base[ , 1])
base[ , 3] = scale(base[ , 3])
base[ , 5] = scale(base[ , 5])
base[ , 11:13] = scale(base[ , 11:13])
set.seed(1)
div = sample.split(base$income, SplitRatio = 0.85)
base_train = subset(base, div == T)
base_test = subset(base, div == F)
library(fBasics)
library(e1071)
library(caTools)
library(caret)  #provide metrics for confusion matrix
clas = naiveBayes(x = base_train[-15, y = base_train$income)
clas = naiveBayes(x = base_train[-15], y = base_train$income)
clas
prev = predict(clas, newdata = base_test[-15])
prev
conf_matrix = table(base_test[ ,15], prev )
conf_matrix
load.image('conf_matrix')
load.image('conf_matrix')
confusionMatrix(conf_matrix)
resutados
resutados
bucerta

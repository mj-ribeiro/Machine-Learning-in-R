View(df)
df['dates'] = NULL
plot(df)
View(df)
plot(df['BVSP.Close'])
plot(df['BVSP.Close'])
View(df)
plot(IBOV, dates)
plot(dates, IBOV)
plot(dates, IBOV, type='l')
rm(df)
fit =  fitted(vol)
summary(vol)
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='Volatilidade do IBOV usando o GARCH(1,1)')
ar = arima(IBOV, order=c(1,1,1))
summary(ar)
ar
fit = fitted(ar)
fit =  fitted(vol)
fit2 = fitted(ar)
plot(fit2)
fit2 = fitted(ar, h=20)
fit2 = forecast(ar)
View(ar)
ar = auto.arima(IBOV, order=c(1,1,1))
#----------- ARIMA
library(tseries)
ar = auto.arima(IBOV, order=c(1,1,1))
library(timeSeries)
ar = auto.arima(IBOV, order=c(1,1,1))
ar = auto.arima(IBOV, lambda='auto')
library(forecast)
ar = auto.arima(IBOV, lambda='auto')
ar
ar = auto.arima(IBOV, c(1,1,1))
ar = auto.arima(IBOV, order=c(1,1,1))
ar = auto.arima(IBOV, start.p = 1)
auto.arima
ar
ar = auto.arima(IBOV, start.p = 1, start.q = 1)
ar
ar = auto.arima(IBOV, start.p = 1, start.q = 2)
ar
ar = auto.arima(IBOV, start.p = 3, start.q = 2)
ar
ar = auto.arima(IBOV, start.p = 2, start.q = 1)
ar
ar = auto.arima(IBOV, start.q = 1)
ar
ar
ar
ar = auto.arima(IBOV, start.q = 1)
ar
ar = auto.arima(IBOV, lambda = 'auto')
ar
fit = forecast(ar, h=10)
plot(fit)
plot(fit, dates)
library(tseries)
library(timeSeries)
library(forecast)   # auto.arima
library(quantmod)
library(fGarch)
getSymbols("^BVSP", src="yahoo", from="2010-01-01")
dates = index(BVSP)
IBOV = BVSP[1:2505 , 4]
View(IBOV)
View(IBOV)
plot( IBOV, type='l')
basicStats(IBOV)
ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV)
IBOV = ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV)
View(IBOV)
IBOV = BVSP[1:2505 , 4]
IBOV = xts(ifelse(is.na(IBOV), mean(IBOV, na.rm=T ), IBOV))
#-------------- GARCH
adf.test(IBOV)
na.omit(IBOV)
IBOV= na.omit(IBOV)
basicStats(IBOV)
adf.test(IBOV)
IBOV[1]
IBOV[2]
IBOV[1:5]
IBOV['new'] = diff(IBOV)
n = length(IBOV)
for (i in n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
for (i in n){
print(i)
}
for (i in 1:n){
print(i)
}
n = length(IBOV)
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
n = length(IBOV)
ret = c( )
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
n = length(IBOV) + 1
ret = c( )
for (i in 1:n) {
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
ret = c( )
for (i in 1:n) {
if (i+1 =< n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret = c( )
for (i in 1:n) {
if (i+1 =< n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret = c( )
for (i in 1:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
ret
ret = rep(0,n )
for (i in 1:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
for (i in 2:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
rm(ret)
rm(ret)
for (i in 2:n) {
if (i+1 < n){
ret[i+1] = log(IBOV[i+1]/IBOV[i])
}
}
for (i in 2:n) {
if (i+1 < n){
ret = log(IBOV[i+1]/IBOV[i])
}
}
View(ret)
for (i in 1:n) {
if (i+1 < n){
ret = log(IBOV[i+1]/IBOV[i])
}
}
log(IBOV[1]/IBOV[1])
log(IBOV[2]/IBOV[1])
log(1/2)
IBOV[1]
IBOV[1]/IBOV[2]
bvcore = coredata(BVSP)
View(bvcore)
class(BVSP)
bvcore = index(dates)
View(bvcore)
bvcore = coredata(BVSP)
colnames(bvcore) = dates
rownames(bvcore) = dates
View(bvcore)
rownames(bvcore) = dates
bvcore[1, 4]
bvcore[1:5, 4]
bvcore[1:5, 4:2]
bvcore[1:5, 2:4]
bvcore[1:5, :4]
bvcore[1:5, 4:4]
bvcore[1:5, 4]
bvcore[1:5, :4]
bvcore[1:5, 4:]
bvcore[1:, 4]
bvcore = data.frame(bvcore)
View(bvcore)
rownames(bvcore) =  dates
View(bvcore)
bvcore[1:4, 1:4]
bvcore[1:4, 4]
names(bvcore)[4] = "IBOV"
View(bvcore)
bvcore[1: , 4]
bvcore[1:4, 4]
rm(ret)
bvcore[1:2505, 4]
IBOV = bvcore[1:2505, 4]
View(IBOV)
rm(IBOV)
bvcore <- bvcore[ ,4, drop=FALSE]
View(bvcore)
bvcore
bvcore[1]
bvcore[1]
bvcore[1]
bvcore[1]
bvcore[1:2]
bvcore[1:2, 1]
bvcore[1:1, 1]
bvcore[1, 1]
bvcore[2, 1]
bvcore[3, 1]
bvcore[3, 1]/bvcore[2, 1]
log(bvcore[3, 1]/bvcore[2, 1])
for (i in 1:n) {
if (i+1 < n){
bvcore$ret = log(bvcore[i+1, 1]/bvcore[i, 1])
}
}
View(bvcore)
for (i in 1:n) {
#if (i+1 < n){
bvcore$ret = log(bvcore[i+1, 1]/bvcore[i, 1])
}
View(bvcore)
for (i in 1:n){
print(i)
}
for (i in 1:n){
print(i)
}
log(bvcore[3, 1]/bvcore[2, 1])
log(bvcore[4, 1]/bvcore[3, 1])
for (i in 1:n) {
#if (i+1 < n){
bvcore$ret[i+1] = log(bvcore[i+1, 1]/bvcore[i, 1])
}
View(bvcore)
plot(bvcore[, 2])
plot(bvcore[ , 2], type='l')
plot(bvcore[ , 2], type='l', col='blue')
plot(bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
)
plot(bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
shapiro.test(bvcore[ , 2])
View(bvcore)
plot(bvcore[ , 2], dates, type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
plot(dates,bvcore[ , 2], type='l',
col='blue', ylab='Returns of IBOVESPA',
main='Evolution of Ibovespa returns')
hist(bvcore[ , 2])
hist(bvcore[ , 2], col = 'lightblue')
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
shapiro.test(bvcore[ , 2])
shapiro.test(bvcore[ , 2]) #The null-hypothesis of this test is that the population is normally distributed.
jarque.bera.test(bvcore[ , 2])
jarque.bera.test(omit.na(bvcore[ , 2]))
jarque.bera.test(na.omit(bvcore[ , 2]))
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
curve(dnorm(x),add=T)
hist(rnorm(1000))
curve(dnorm(x),add=T)
hist(rnorm(1000))
curve(dnorm(x),add=T)
hist(rnorm(1000), ylim=c(0,0.5))
curve(dnorm(x),add=T)
hist(rnorm(1000), probability = T)
curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', probability = T)
curve(dnorm(x),add=T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns')
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', density = T)
hist(bvcore[ , 2], col = 'lightblue', main = 'Histogram of IBOVESPA returns', probability = T)
curve(dnorm(x),add=T)
basicStats(bvcore[ , 2])
t.test(bvcore[ , 2], mu=0)
adf.test(bvcore[ , 2])
adf.test(na.omit(bvcore[ , 2]))
adf.test(na.omit(bvcore[ , 2]) )
bvcore[ , 2] = ifelse(is.na(bvcore[ , 2]), mean(bvcore[ , 2], na.rm=T ), bvcore[ , 2])
View(bvcore)
basicStats(bvcore[ , 2])
vol = garchFit(bvcore[ , 2] ~garch(1, 1), trace = FALSE)
summary(vol)
basicStats(bvcore[ , 2])
summary(vol)
fit =  fitted(vol)
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='Volatilidade do IBOV usando o GARCH(1,1)')
plot(fit, type='l', col='blue', ylab='Volatilidade',
main='IBOV volatility using GARCH(1,1)')
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
fit = forecast(ar, h=10)
plot(fit)
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
fit = forecast(ar, h=30)
plot(fit)
library(mFilter)
filt = hpfilter(bvcore[ , 2])
filt = hpfilter(bvcore[ , 2], freq = 1600)
plot(filt)
#----------- ARIMA
acf(bvcore[ , 2])
pacf(bvcore[ , 2])
par(mfrow=c(1,2))
acf(bvcore[ , 2])
pacf(bvcore[ , 2])
acf(bvcore[ , 2], main='Return IBOV')
#----------- ARIMA
par(mfrow=c(1,2))
acf(bvcore[ , 2], main='Autocorrelation of IBOV returns')
pacf(bvcore[ , 2], main='Partial autocorrelation of IBOV returns')
#----------- ARIMA
window()
#----------- ARIMA
windows()
windows()
par(mfrow=c(1,2))
acf(bvcore[ , 2], main='Autocorrelation of IBOV returns')
pacf(bvcore[ , 2], main='Partial autocorrelation of IBOV returns')
ar = auto.arima(bvcore[ , 2], lambda = 'auto')
ar
54.5+543.95
54.5+543.95+1.55
1000*1.05
1000*0.05
5458.2+10*82.7
(5458.2+10*82.7)/76
35*94.4
99.8*10+96.55*13
(99.8*10+96.55*13)/23
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86)/48
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 20*82)/68
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 30*82)/68
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 30*82)/78
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 40*82)/78
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 40*82)/88
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 40*75)/88
(99.8*10+96.55*13+10*85.5+5*85.7+5*85.6+5*86+ 40*70)/88
3.6*4
15/9
400/1-0.6
400/(1-0.6)
160+0.6*900
1/(1-0.6)
13.85-13.42
0.43*60
13.9-13.42
13.9-13.42)*60
(13.9-13.42)*60
(13.92-13.42)*60
835.20-805.44
4^4
3^4
81/256
1000*(81/256)
1000/(1000*(81/256))
(1000/(1000*(81/256)))^(1/4)
2/3
1.333*3
4/3
1000*(81/256)*2^(-4)
256/81
(256/81)^(-1/4)
1000*(81/256)*1^(-4)
1000*(81/256)*2^(-4)
1000*(81/256)*1^(-4)
1000^(1/4)
1000^(1/4)*316.4^(3/4)
332/20
332/23.98
332/13.93
library(sn)
library(PerformanceAnalytics)
library(car)
library(tseries)
library(forecast)
library(fImport)
library(timeSeries)
data=read.table(file.choose(),header=T)
index=is.na(data[,2])==F & is.na(data[,3])==F & is.na(data[,4])==F
data=data[index,]
datas=as.matrix(data[,1])
ibov=data[,2]
sp500=data[,3]
petr4=data[,4]
##### tranformar em ts
tsibov=timeSeries(data=ibov,charvec=datas,format='%d/%m/%Y')
tssp500=timeSeries(data=sp500,charvec=datas,format='%d/%m/%Y')
## plots
windows()
par(mfrow=c(1,2))
plot(tsibov, ylab='IBOV')
plot(tssp500, ylab='SP500')
sp500f=as.zoo(tssp500)
sibovf=as.zoo(tsibov)
ibov
sp500
### convert to log returns
#other form to calculate returns
ret = log(tsibov/lag(tsibov))
hist(ret, breaks=25, col="slateblue")
table.Stats(ret)
par(mfrow=c(1,2))
dlibov = diff(log(ibov))
dlsp500 = diff(log(sp500))
hist(dlsp500, breaks=25, col="slateblue")
table.Stats(dlsp500)
hist(dlibov, breaks=25, col="slateblue")
table.Stats(dlibov)
# Can also use PerformanceAnalytics functions
chart.Histogram(dlsp500, methods="add.normal")
chart.Histogram(dlibov, methods="add.normal")
#
# QQ-plots and tests for normality
#
ks.test(dlsp500,'pnorm',mean(dlsp500),sd(dlsp500))
sn.dlibov.fit = sn.mple(y=dlibov)
sn.dlibov.fit
qqPlot(dlibov, dist="sn", xi=sn.dlibov.fit$cp[1], omega=sn.dlibov.fit$cp[2], alpha=sn.dlibov.fit$cp[3])
library(ghyp)
univariate.ghyp <- ghyp(lambda=2, alpha.bar=0.1, mu=0, sigma=1, gamma=0)
univariate.ghyp <- ghyp(lambda=2, chi=1, psi=0.5, mu=0, sigma=1, gamma=0)
par(mfrow=c(2, 1))
xvalue <- seq(-4, 4, length = 500)
quantiles=xvalue
plot(quantiles, dghyp(xvalue , univariate.ghyp))
plot(quantiles, pghyp(xvalue , univariate.ghyp))
fitted.returns.ghyp <- fit.ghypuv(data=dlsp500,silent=TRUE)
hist(fitted.returns.ghyp,log.hist=T,nclass=30,plot.legend=F,ghyp.col="blue")
qqghyp(fitted.returns.ghyp,plot.legend=T,legend.cex=0.7)
par(mfrow=c(3,1))
ts.plot(dlibov,col="blue", main="dlibov")
ts.plot(dlibov^2,col="blue", main="dlibov^2")
ts.plot(abs(dlibov),col="blue", main="abs(dlibov)")
par(mfrow=c(3,1))
Acf(dlibov, lwd=2)
Acf(dlibov^2, lwd=2)
Acf(abs(dlibov), lwd=2)
par(mfrow=c(1,1))
par(mfrow=c(3,1))
ts.plot(dlsp500,col="blue", main="dlibov")
ts.plot(dlsp500^2,col="blue", main="dlibov^2")
ts.plot(abs(dlsp500),col="blue", main="abs(dlibov)")
par(mfrow=c(3,1))
ts.plot(dlsp500,col="blue", main="dlibov")
ts.plot(dlsp500^2,col="blue", main="dlibov^2")
ts.plot(abs(dlsp500),col="blue", main="abs(dlibov)")
q()
setwd("D:/Git projects/ML in R")
library(rpart)
library(rpart.plot)
library(fBasics)
library(caTools)  # split data
library(caret)    #provide metrics for confusion matrix
#-------------- Read data
base = read.csv('census.csv')
base$X = NULL
attach(base)
#------------------ CATEGORIZING DATA
base$sex = factor(base$sex, levels = unique(base$sex), labels = c(1, 0))
base$workclass = factor(base$workclass, levels = c(' Federal-gov', ' Local-gov', ' Private', ' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'), labels = c(1, 2, 3, 4, 5, 6, 7))
base$education = factor(base$education, levels = c(' 10th', ' 11th', ' 12th', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th', ' Assoc-acdm', ' Assoc-voc', ' Bachelors', ' Doctorate', ' HS-grad', ' Masters', ' Preschool', ' Prof-school', ' Some-college'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16))
base$marital.status = factor(base$marital.status, levels = c(' Divorced', ' Married-AF-spouse', ' Married-civ-spouse', ' Married-spouse-absent', ' Never-married', ' Separated', ' Widowed'), labels = c(1, 2, 3, 4, 5, 6, 7))
base$occupation = factor(base$occupation, levels = c(' Adm-clerical', ' Armed-Forces', ' Craft-repair', ' Exec-managerial', ' Farming-fishing', ' Handlers-cleaners', ' Machine-op-inspct', ' Other-service', ' Priv-house-serv', ' Prof-specialty', ' Protective-serv', ' Sales', ' Tech-support', ' Transport-moving'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
base$relationship = factor(base$relationship, levels = c(' Husband', ' Not-in-family', ' Other-relative', ' Own-child', ' Unmarried', ' Wife'), labels = c(1, 2, 3, 4, 5, 6))
base$race = factor(base$race, levels = c(' Amer-Indian-Eskimo', ' Asian-Pac-Islander', ' Black', ' Other', ' White'), labels = c(1, 2, 3, 4, 5))
base$native.country = factor(base$native.country, levels = c(' Cambodia', ' Canada', ' China', ' Columbia', ' Cuba', ' Dominican-Republic', ' Ecuador', ' El-Salvador', ' England', ' France', ' Germany', ' Greece', ' Guatemala', ' Haiti', ' Holand-Netherlands', ' Honduras', ' Hong', ' Hungary', ' India', ' Iran', ' Ireland', ' Italy', ' Jamaica', ' Japan', ' Laos', ' Mexico', ' Nicaragua', ' Outlying-US(Guam-USVI-etc)', ' Peru', ' Philippines', ' Poland', ' Portugal', ' Puerto-Rico', ' Scotland', ' South', ' Taiwan', ' Thailand', ' Trinadad&Tobago', ' United-States', ' Vietnam', ' Yugoslavia'), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41))
base$income = factor(base$income, levels = c(' <=50K', ' >50K'), labels = c(0, 1))
#---------------- SCALING
base[ , 1] = scale(base[ , 1])
base[ , 3] = scale(base[ , 3])
base[ , 5] = scale(base[ , 5])
base[ , 11:13] = scale(base[ , 11:13])
#----------------------- TRAIN AND TEST
set.seed(1)
div = sample.split(base$income, SplitRatio = 0.85)
base_train = subset(base, div == T)
base_test = subset(base, div == F)
# ------------ Algorithm
clas = rpart(formula=income ~ ., data = base_train)
# ---------- Plot
rpart.plot(clas)
prev
prev = predict(clas, newdata = base_test, type = 'class')
prev
view(prev)
View(prev)
df = read.csv('naive_base.csv')
df = df[df$risco != 'moderado',  ]
clas = glm(risco ~ . , family = binomial, data=df )
clas
View(df)
prev = predict(clas, df2, type='response')  # type make probabilities appear
prev
df = read.csv('naive_base.csv')
df = df[df$risco != 'moderado',  ]
clas = glm(risco ~ . , family = binomial, data=df )
clas
historia = c('boa', 'ruim')
divida = c('alta', 'alta')
garantias = c('nenhuma', 'adequada')
renda = c('acima_35', '0_15')
df2 = data.frame(historia, divida, garantias, renda)
prev = predict(clas, df2, type='response')  # type make probabilities appear
prev
View(df2)
